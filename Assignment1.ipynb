{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a90dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39888ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read:  1133\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "fileMap = {}\n",
    "x = 0\n",
    "for filename in os.listdir('./Files'):\n",
    "    try:\n",
    "        f = open(\"./Files/\"+filename, \"r\")\n",
    "        files.append(f.read())\n",
    "        fileMap[x] = filename\n",
    "        x += 1\n",
    "    except:\n",
    "        f = open(\"./Files/\"+filename, \"rb\")\n",
    "        files.append(f.read().decode('utf-8', 'backslashreplace'))\n",
    "        fileMap[x] = filename\n",
    "        x += 1\n",
    "        \n",
    "print(\"Files read: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffcf51",
   "metadata": {},
   "source": [
    "# Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b88d2",
   "metadata": {},
   "source": [
    "## a) Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15a6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10310dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(file):\n",
    "    # Keep only alphabets and numbers\n",
    "    file = re.sub(r'[^a-zA-Z0-9]+', ' ', file)\n",
    "    # Convert to lower case:\n",
    "    file = file.lower()\n",
    "    # Stopword removal\n",
    "    file = [word for word in file.split() if word not in stopWords]\n",
    "    # Lemmatize\n",
    "    file = [lemmatizer.lemmatize(word) for word in file]    #lemmatize the words\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6b9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitFiles = []\n",
    "for file in files:\n",
    "    splitFiles.append(set(preProcess(file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0803c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03f8b8e9",
   "metadata": {},
   "source": [
    "## b) Unigram Inverted Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db40f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "invertedIndex = {}\n",
    "for idx, file in enumerate(splitFiles): # For each file. numbered 0-n\n",
    "    for word in file: # for each unique word in that file.\n",
    "        if word not in invertedIndex: # insert into dictionary the document index.\n",
    "            invertedIndex[word] = []\n",
    "        invertedIndex[word].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90782c6",
   "metadata": {},
   "source": [
    "## c) Boolean Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39957fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(x, index):\n",
    "    try:\n",
    "        xList = index[x]\n",
    "        return xList\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e48bb",
   "metadata": {},
   "source": [
    "#### 1. x OR y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51785a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xORy(xList, yList):\n",
    "    xLen = len(xList)\n",
    "    yLen = len(yList)\n",
    "    \n",
    "    # if one is empty return the other.\n",
    "    if xLen == 0:\n",
    "        return (yList, 1)\n",
    "    if yLen == 0:\n",
    "        return (xList, 1)\n",
    "    \n",
    "    # merge both\n",
    "    comparisons = 0\n",
    "    a = 0\n",
    "    b = 0\n",
    "    ans = set()\n",
    "    while a < xLen and b < yLen:\n",
    "        # Compare values\n",
    "        if xList[a] <= yList[b]:\n",
    "            ans.add(xList[a])\n",
    "            a += 1\n",
    "        else:\n",
    "            ans.add(yList[b])\n",
    "            b += 1\n",
    "        \n",
    "        comparisons += 1\n",
    "            \n",
    "    # if some values remaining from xList then add them.\n",
    "    while a < xLen:\n",
    "        ans.add(xList[a])\n",
    "        a += 1\n",
    "\n",
    "    # if some values remaining from yList then add them.\n",
    "    while b < yLen:\n",
    "        ans.add(yList[b])\n",
    "        b += 1\n",
    "        \n",
    "    return (list(ans), comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ee737",
   "metadata": {},
   "source": [
    "#### 2. x AND y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2001e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xANDy(xList, yList):\n",
    "    xLen = len(xList)\n",
    "    yLen = len(yList)\n",
    "    \n",
    "    # if one is empty answer is empty.\n",
    "    if xLen == 0:\n",
    "        return (xList, 1)\n",
    "    if yLen == 0:\n",
    "        return (yList, 1)\n",
    "    \n",
    "    # merge both\n",
    "    comparisons = 0\n",
    "    a = 0\n",
    "    b = 0\n",
    "    ans = set()\n",
    "    while a < xLen and b < yLen:\n",
    "        # Compare values\n",
    "        if xList[a] < yList[b]:\n",
    "            a += 1\n",
    "        elif xList[a] > yList[b]:\n",
    "            b += 1\n",
    "        else:\n",
    "            ans.add(yList[b])\n",
    "            a += 1\n",
    "            b += 1\n",
    "        \n",
    "        comparisons += 1\n",
    "        \n",
    "    return (list(ans), comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c43e9b",
   "metadata": {},
   "source": [
    "#### 3. x AND NOT y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b578399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xANDNOTy(xList, yList):\n",
    "    xLen = len(xList)\n",
    "    yLen = len(yList)\n",
    "    \n",
    "    # if x is empty return empty\n",
    "    if xLen == 0:\n",
    "        return ([], 1)\n",
    "    # if y is empty return x\n",
    "    if yLen == 0:\n",
    "        return (xList, 1)\n",
    "    \n",
    "    # merge both\n",
    "    comparisons = 0\n",
    "    a = 0\n",
    "    b = 0\n",
    "    ans = set()\n",
    "    while a < xLen and b < yLen:\n",
    "        # Compare values\n",
    "        if xList[a] < yList[b]:\n",
    "            ans.add(xList[a])\n",
    "            a += 1\n",
    "        elif xList[a] > yList[b]:\n",
    "            b += 1\n",
    "        else:\n",
    "            a += 1\n",
    "            b += 1\n",
    "        \n",
    "        comparisons += 1\n",
    "        \n",
    "    return (list(ans), comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc525b97",
   "metadata": {},
   "source": [
    "#### 4. x OR NOT y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15e9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xORNOTy(xList, yList):\n",
    "    xLen = len(xList)\n",
    "    yLen = len(yList)\n",
    "    \n",
    "    ans = set(range(len(files)))\n",
    "    comparisons = 0\n",
    "    for yW in yList:\n",
    "        if yW not in xList: # remove everyword in yList and not in xList\n",
    "            ans.remove(yW)\n",
    "        comparisons += 1\n",
    "    \n",
    "    return (list(ans), comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d9ba3",
   "metadata": {},
   "source": [
    "## d) Queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9a09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeInput():\n",
    "    query = input(\"Enter string without operators: \")\n",
    "    op = input(\"Enter operators (comma seperated): \")\n",
    "    op = op.split(\",\")\n",
    "    op = [x.strip() for x in op]\n",
    "    query = preProcess(query)\n",
    "    print(query, \"\\n\", op)\n",
    "    return(query, op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eef81d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(queryString, operators):\n",
    "    if len(queryString) == 0:\n",
    "        print(\"Total number of documents: \", 0)\n",
    "        print(\"Total number of comparisons: \", 0)\n",
    "        return\n",
    "    \n",
    "    xList = getList(queryString[0], invertedIndex)\n",
    "    opIdx = 0\n",
    "    nextListIdx = 1\n",
    "    \n",
    "    # Read query left to right and get results\n",
    "    totalComps = 0\n",
    "    while nextListIdx < len(queryString) and opIdx < len(operators):\n",
    "        yList = getList(queryString[nextListIdx], invertedIndex)\n",
    "        \n",
    "        if operators[opIdx] == 'OR':\n",
    "            res = xORy(xList, yList)\n",
    "            xList = res[0]\n",
    "            totalComps += res[1]\n",
    "        elif operators[opIdx] == 'AND':\n",
    "            res = xANDy(xList, yList)\n",
    "            xList = res[0]\n",
    "            totalComps += res[1]\n",
    "        elif operators[opIdx] == 'AND NOT':\n",
    "            res = xANDNOTy(xList, yList)\n",
    "            xList = res[0]\n",
    "            totalComps += res[1]\n",
    "        elif operators[opIdx] == 'OR NOT':\n",
    "            res = xORNOTy(xList, yList)\n",
    "            xList = res[0]\n",
    "            totalComps += res[1]\n",
    "        else:\n",
    "            print(\"Wrong operator found\")\n",
    "            break\n",
    "        \n",
    "        opIdx += 1\n",
    "        nextListIdx += 1\n",
    "    \n",
    "    print(\"Total documents: \", len(xList))\n",
    "    print(\"Total comparisons: \", totalComps)\n",
    "    return xList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "901051fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter string without operators: lion stood thoughtfully for the moment\n",
      "Enter operators (comma seperated): OR,OR, OR\n",
      "['lion', 'stood', 'thoughtfully', 'moment'] \n",
      " ['OR', 'OR', 'OR']\n"
     ]
    }
   ],
   "source": [
    "(q, op) = takeInput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e5e647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents:  211\n",
      "Total comparisons:  353\n"
     ]
    }
   ],
   "source": [
    "fileIdx = getResults(q, op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a224b38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028 commutin.jok\n",
      "1031 grail.txt\n",
      "523 suicide2.txt\n",
      "524 cartoon.laws\n",
      "531 radiolaf.hum\n",
      "534 hackingcracking.txt\n",
      "535 snapple.rum\n",
      "22 kanalx.txt\n",
      "25 bmdn01.txt\n",
      "540 myheart.hum\n",
      "32 tnd.1\n",
      "33 annoy.fascist\n",
      "544 dingding.hum\n",
      "1056 luggage.hum\n",
      "1057 facedeth.txt\n",
      "38 stone.hum\n",
      "558 flux_fix.txt\n",
      "560 hackmorality.txt\n",
      "50 jayjay.txt\n",
      "562 badday.hum\n",
      "565 luvstory.txt\n",
      "567 mash.hum\n",
      "569 dthought.txt\n",
      "1081 homebrew.txt\n",
      "571 idr2.txt\n",
      "1087 conan.txt\n",
      "1089 lion.jok\n",
      "1091 popmusi.hum\n",
      "1092 montpyth.hum\n",
      "70 insults1.txt\n",
      "73 reasons.txt\n",
      "76 gd_gal.txt\n",
      "589 golnar.txt\n",
      "1100 phorse.hum\n",
      "80 incarhel.hum\n",
      "592 tfepisod.hum\n",
      "595 econridl.fun\n",
      "597 nigel10.txt\n",
      "86 sfmovie.txt\n",
      "1110 soleleer.hum\n",
      "602 news.hum\n",
      "604 nihgel_8.9\n",
      "94 bw-phwan.hat\n",
      "95 stuf11.txt\n",
      "96 sw_err.txt\n",
      "97 pizzawho.hum\n",
      "606 prac3.jok\n",
      "608 prac2.jok\n",
      "1120 dead3.txt\n",
      "101 butwrong.hum\n",
      "1121 fascist.txt\n",
      "615 gd_ql.txt\n",
      "105 calculus.txt\n",
      "617 christop.int\n",
      "107 iremember\n",
      "108 stuf10.txt\n",
      "621 computer.txt\n",
      "110 oliver02.txt\n",
      "1131 xibovac.txt\n",
      "113 top10st2.txt\n",
      "625 is_story.txt\n",
      "118 m0dzmen.hum\n",
      "631 clancy.txt\n",
      "637 quest.hum\n",
      "638 nukewar.txt\n",
      "126 cuchy.hum\n",
      "128 anim_lif.txt\n",
      "642 cartoon.law\n",
      "644 solders.hum\n",
      "646 moose.txt\n",
      "136 various.txt\n",
      "142 lbinter.hum\n",
      "655 manners.txt\n",
      "658 booze1.fun\n",
      "149 lifeonledge.txt\n",
      "669 nigel.10\n",
      "670 indgrdn.txt\n",
      "160 ghostfun.hum\n",
      "161 boneles2.txt\n",
      "162 smurfkil.hum\n",
      "165 peatchp.hum\n",
      "680 prac1.jok\n",
      "681 allusion\n",
      "687 onetoone.hum\n",
      "688 epitaph\n",
      "694 pro-fact.hum\n",
      "183 nigel.3\n",
      "695 bw.txt\n",
      "191 quack26.txt\n",
      "703 coyote.txt\n",
      "704 cabbage.txt\n",
      "710 devils.jok\n",
      "199 murphys.txt\n",
      "206 tpquotes.txt\n",
      "209 terrmcd'.hum\n",
      "215 wacky.ani\n",
      "731 pracjoke.txt\n",
      "732 oldeng.hum\n",
      "738 caesardr.sal\n",
      "230 drinks.gui\n",
      "745 initials.rid\n",
      "234 bbh_intv.txt\n",
      "237 passenge.sim\n",
      "751 prac4.jok\n",
      "245 exam.50\n",
      "247 nigel.5\n",
      "760 let.go\n",
      "761 puzzles.jok\n",
      "252 nigel.2\n",
      "767 ivan.hum\n",
      "775 moore.txt\n",
      "265 lozerzon.hum\n",
      "778 shuttleb.hum\n",
      "779 art-fart.hum\n",
      "782 epi_.txt\n",
      "783 nameisreo.txt\n",
      "272 passage.hum\n",
      "784 english.txt\n",
      "786 timetr.hum\n",
      "788 epi_tton.txt\n",
      "277 lions.cat\n",
      "282 cartoon_.txt\n",
      "796 gown.txt\n",
      "286 meinkamp.hum\n",
      "802 filmgoof.txt\n",
      "809 oxymoron.jok\n",
      "298 female.jok\n",
      "299 drunk.txt\n",
      "301 mel.txt\n",
      "303 chickenheadbbs.txt\n",
      "817 pukeprom.jok\n",
      "306 aeonint.txt\n",
      "819 a_tv_t-p.com\n",
      "821 anime.lif\n",
      "310 doggun.sto\n",
      "311 scratchy.txt\n",
      "312 classicm.hum\n",
      "316 bitnet.txt\n",
      "829 childhoo.jok\n",
      "321 top10.txt\n",
      "322 misc.1\n",
      "324 ambrose.bie\n",
      "327 whoops.hum\n",
      "331 lif&love.hum\n",
      "336 murphy_l.txt\n",
      "848 humor9.txt\n",
      "849 policpig.hum\n",
      "342 consp.txt\n",
      "343 psycho.txt\n",
      "856 vonthomp\n",
      "861 reeves.txt\n",
      "862 kaboom.hum\n",
      "353 cybrtrsh.txt\n",
      "354 wedding.hum\n",
      "865 pepsideg.txt\n",
      "357 oliver.txt\n",
      "872 mundane.v2\n",
      "360 namaste.txt\n",
      "876 fuckyou2.txt\n",
      "367 throwawa.hum\n",
      "371 onetotwo.hum\n",
      "885 socecon.hum\n",
      "887 mindvox\n",
      "376 wimptest.txt\n",
      "382 lawyer.jok\n",
      "895 eskimo.nel\n",
      "384 jc-elvis.inf\n",
      "391 rns_ency.txt\n",
      "903 pepper.txt\n",
      "906 barney.txt\n",
      "396 quux_p.oem\n",
      "397 llong.hum\n",
      "910 cogdis.txt\n",
      "403 worldend.hum\n",
      "404 progrs.gph\n",
      "405 engineer.hum\n",
      "918 petshop\n",
      "407 letgosh.txt\n",
      "922 quotes.txt\n",
      "923 maecenas.hum\n",
      "924 mailfrag.hum\n",
      "926 gas.txt\n",
      "415 insult.lst\n",
      "930 japantv.txt\n",
      "932 mlverb.hum\n",
      "933 ukunderg.txt\n",
      "421 dromes.txt\n",
      "427 three.txt\n",
      "431 practica.txt\n",
      "944 dead4.txt\n",
      "434 candy.txt\n",
      "946 minn.txt\n",
      "441 hecomes.jok\n",
      "958 mcd.txt\n",
      "452 deep.txt\n",
      "457 collected_quotes.txt\n",
      "971 valujet.txt\n",
      "976 dead5.txt\n",
      "979 lifeimag.hum\n",
      "992 episimp2.txt\n",
      "484 beesherb.txt\n",
      "485 psych_pr.quo\n",
      "1000 beauty.tm\n",
      "490 lion.txt\n",
      "492 msorrow\n",
      "1006 strine.txt\n",
      "498 cmu.share\n",
      "1010 b-2.jok\n",
      "504 marriage.hum\n",
      "508 cookie.1\n",
      "511 wagon.hum\n"
     ]
    }
   ],
   "source": [
    "for f in fileIdx:\n",
    "    print(f, fileMap[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61011ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter string without operators: telephone,paved, roads\n",
      "Enter operators (comma seperated): OR NOT, AND NOT\n",
      "['telephone', 'paved', 'road'] \n",
      " ['OR NOT', 'AND NOT']\n",
      "Total documents:  996\n",
      "Total comparisons:  1137\n"
     ]
    }
   ],
   "source": [
    "(q, op) = takeInput()\n",
    "x = getResults(q, op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d62c8abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 naivewiz.hum\n",
      "1 lawsuniv.hum\n",
      "2 potty.txt\n",
      "3 adrian_e.faq\n",
      "4 pecker.txt\n",
      "5 miranda.hum\n",
      "6 imbecile.txt\n",
      "7 fish.rec\n",
      "8 btscke04.des\n",
      "9 poli_t.ics\n",
      "10 cucumber.jok\n",
      "11 lp-assoc.txt\n",
      "12 brownie.rec\n",
      "13 egglentl.vgn\n",
      "14 chainltr.txt\n",
      "15 murph.jok\n",
      "16 farsi.txt\n",
      "17 goforth.hum\n",
      "18 socks.drx\n",
      "19 fed.txt\n",
      "20 htswfren.txt\n",
      "22 kanalx.txt\n",
      "23 wood\n",
      "24 curse.txt\n",
      "25 bmdn01.txt\n",
      "26 vegan.rcp\n",
      "27 pbcookie.des\n",
      "28 prover_w.iso\n",
      "29 1st_aid.txt\n",
      "30 law.sch\n",
      "31 merry.txt\n",
      "32 tnd.1\n",
      "33 annoy.fascist\n",
      "34 spoonlis.txt\n",
      "36 top10st1.txt\n",
      "37 takenote.jok\n",
      "38 stone.hum\n",
      "39 planetzero.txt\n",
      "40 transp.txt\n",
      "41 newmex.hum\n",
      "42 prayer.hum\n",
      "43 middle.age\n",
      "44 nukwaste\n",
      "45 phxbbs-m.txt\n",
      "46 avengers.lis\n",
      "47 woolly_m.amm\n",
      "48 blkbean.txt\n",
      "50 jayjay.txt\n",
      "51 test.hum\n",
      "52 gack!.txt\n",
      "53 nukewar.jok\n",
      "54 eandb.drx\n",
      "55 margos.txt\n",
      "56 btscke05.des\n",
      "57 cgs_lst.txt\n",
      "59 jimhood.txt\n",
      "60 cereal.txt\n",
      "61 oasis\n",
      "62 hell.txt\n",
      "63 fwksfun.hum\n",
      "64 cheapin.la\n",
      "65 corporat.txt\n",
      "66 planeget.hum\n",
      "67 jokeju07.txt\n",
      "68 btcisfre.hum\n",
      "69 dead-r\n",
      "71 pure.mat\n",
      "72 signatur.jok\n",
      "73 reasons.txt\n",
      "74 shrink.news\n",
      "75 smurf_co.txt\n",
      "76 gd_gal.txt\n",
      "77 smokers.txt\n",
      "78 welfare\n",
      "79 killer.hum\n",
      "80 incarhel.hum\n",
      "81 german.aut\n",
      "82 moslem.txt\n",
      "83 raven.hum\n",
      "84 mtm.hum\n",
      "86 sfmovie.txt\n",
      "88 whitbred.txt\n",
      "89 college.hum\n",
      "90 sinksub.txt\n",
      "91 wagit.txt\n",
      "92 how2bgod.txt\n",
      "93 lost.txt\n",
      "94 bw-phwan.hat\n",
      "96 sw_err.txt\n",
      "97 pizzawho.hum\n",
      "98 pournell.spo\n",
      "99 yogurt.asc\n",
      "101 butwrong.hum\n",
      "102 btaco.txt\n",
      "103 letter_f.sch\n",
      "105 calculus.txt\n",
      "106 free-cof.fee\n",
      "107 iremember\n",
      "109 p-law.hum\n",
      "110 oliver02.txt\n",
      "111 ghostsch.hum\n",
      "112 zgtoilet.txt\n",
      "113 top10st2.txt\n",
      "114 hate.hum\n",
      "115 memo.hum\n",
      "116 radexposed.txt\n",
      "117 fajitas.rcp\n",
      "118 m0dzmen.hum\n",
      "119 hamburge.nam\n",
      "120 coke1\n",
      "121 bakebred.txt\n",
      "122 wonton.txt\n",
      "123 opinion.hum\n",
      "125 llamas.txt\n",
      "127 pat.txt\n",
      "128 anim_lif.txt\n",
      "129 electric.txt\n",
      "130 girlspeak.txt\n",
      "131 polly_.new\n",
      "132 miamimth.txt\n",
      "133 wkrp.epi\n",
      "134 yuppies.hum\n",
      "135 investi.hum\n",
      "136 various.txt\n",
      "137 coffee.faq\n",
      "138 lozers\n",
      "139 bank.rob\n",
      "140 texican.lex\n",
      "141 un.happy\n",
      "143 c0dez.txt\n",
      "144 bw-summe.hat\n",
      "145 ookpik.hum\n",
      "146 bad.jok\n",
      "147 limerick.jok\n",
      "148 btscke02.des\n",
      "149 lifeonledge.txt\n",
      "150 elephant.fun\n",
      "152 beer.txt\n",
      "153 horflick.txt\n",
      "154 old.txt\n",
      "155 mead.rcp\n",
      "156 finalexm.hum\n",
      "157 docdict.txt\n",
      "158 math.far\n",
      "159 mensroom.jok\n",
      "160 ghostfun.hum\n",
      "163 phunatdi.ana\n",
      "164 gd_flybd.txt\n",
      "165 peatchp.hum\n",
      "166 one.par\n",
      "168 msfields.txt\n",
      "169 cokeform.txt\n",
      "170 firstaid.inf\n",
      "171 malechem.txt\n",
      "173 rec.por\n",
      "174 novel.hum\n",
      "176 thermite.ana\n",
      "177 ganamembers.txt\n",
      "178 exidy.txt\n",
      "179 mov_rail.txt\n",
      "180 who.txt\n",
      "181 college.sla\n",
      "182 post.nuc\n",
      "184 shuimai.txt\n",
      "185 oakwood.txt\n",
      "186 btscke03.des\n",
      "187 chung.iv\n",
      "189 bb\n",
      "190 recipe.010\n",
      "192 nigel.4\n",
      "193 lobquad.hum\n",
      "194 sungenu.hum\n",
      "195 jeffie.heh\n",
      "196 hierarch.txt\n",
      "197 popmach\n",
      "198 fusion.gal\n",
      "199 murphys.txt\n",
      "200 insult\n",
      "201 insure.hum\n",
      "202 fusion.sup\n",
      "203 misery.hum\n",
      "204 alflog.txt\n",
      "205 tpquote2.txt\n",
      "206 tpquotes.txt\n",
      "207 televisi.hum\n",
      "208 psalm_re.aga\n",
      "209 terrmcd'.hum\n",
      "210 newcoke.txt\n",
      "211 reddwarf.sng\n",
      "213 btscke01.des\n",
      "214 coffeebeerwomen.txt\n",
      "215 wacky.ani\n",
      "216 drinkrul.jok\n",
      "217 tuflife.txt\n",
      "218 spydust.hum\n",
      "219 justify\n",
      "220 the_ant.txt\n",
      "221 hitchcok.txt\n",
      "222 oxymoron.txt\n",
      "223 vegkill.txt\n",
      "224 penisprt.txt\n",
      "225 fuck!.txt\n",
      "226 soccer.txt\n",
      "227 blood.txt\n",
      "228 blooprs1.asc\n",
      "231 brdpudd.des\n",
      "233 deadlysins.txt\n",
      "236 turing.shr\n",
      "237 passenge.sim\n",
      "238 lifeinfo.hum\n",
      "239 mrscienc.hum\n",
      "240 arthriti.txt\n",
      "241 bbc_vide.cat\n",
      "242 recipe.011\n",
      "243 math.2\n",
      "244 texican.dic\n",
      "245 exam.50\n",
      "246 mtv.asc\n",
      "249 answers\n",
      "250 hi.tec\n",
      "251 solviets.hum\n",
      "252 nigel.2\n",
      "254 grospoem.txt\n",
      "255 crzycred.lst\n",
      "256 docspeak.txt\n",
      "257 quantity.001\n",
      "258 tickmoon.hum\n",
      "260 aggie.txt\n",
      "262 cartoon_laws.txt\n",
      "263 coladrik.fun\n",
      "264 ludeinfo.hum\n",
      "266 cookbkly.how\n",
      "267 test.jok\n",
      "268 rabbit.txt\n",
      "269 hitler.59\n",
      "270 good.txt\n",
      "271 recipe.005\n",
      "272 passage.hum\n",
      "273 toxcwast.hum\n",
      "274 batrbred.txt\n",
      "275 hotpeper.txt\n",
      "276 cooking.fun\n",
      "277 lions.cat\n",
      "278 twinkies.jok\n",
      "280 macsfarm.old\n",
      "281 bimg.prn\n",
      "282 cartoon_.txt\n",
      "283 horoscope.txt\n",
      "284 recipe.002\n",
      "285 horoscop.jok\n",
      "286 meinkamp.hum\n",
      "287 chili.txt\n",
      "288 prover.wisom\n",
      "289 laws.txt\n",
      "290 flowchrt\n",
      "291 chinese.txt\n",
      "292 pickup.lin\n",
      "293 aussie.lng\n",
      "294 analogy.hum\n",
      "295 silverclaws.txt\n",
      "296 mr.rogers\n",
      "297 baklava.des\n",
      "298 female.jok\n",
      "299 drunk.txt\n",
      "300 defectiv.hum\n",
      "301 mel.txt\n",
      "303 chickenheadbbs.txt\n",
      "304 studentb.txt\n",
      "306 aeonint.txt\n",
      "307 alabama.txt\n",
      "308 cuisine.txt\n",
      "309 number.killer\n",
      "310 doggun.sto\n",
      "312 classicm.hum\n",
      "313 quantum.jok\n",
      "314 proposal.jok\n",
      "317 argotdic.txt\n",
      "318 forsooth.hum\n",
      "319 ads.txt\n",
      "320 looser.hum\n",
      "321 top10.txt\n",
      "323 qttofu.vgn\n",
      "324 ambrose.bie\n",
      "325 atombomb.hum\n",
      "327 whoops.hum\n",
      "328 zuccmush.sal\n",
      "329 gd_hhead.txt\n",
      "330 quotes.jok\n",
      "331 lif&love.hum\n",
      "332 manilla.hum\n",
      "333 symbol.hum\n",
      "334 humatra.txt\n",
      "336 murphy_l.txt\n",
      "338 bbq.txt\n",
      "339 hacktest.txt\n",
      "340 fartting.txt\n",
      "341 robot.tes\n",
      "343 psycho.txt\n",
      "344 get.drunk.cheap\n",
      "345 turkey.fun\n",
      "346 memory.hum\n",
      "347 excuse30.txt\n",
      "348 calif.hum\n",
      "349 coffee.txt\n",
      "350 frogeye1.sal\n",
      "351 koans.txt\n",
      "352 choco-ch.ips\n",
      "353 cybrtrsh.txt\n",
      "354 wedding.hum\n",
      "355 blackapp.hum\n",
      "356 comrevi1.hum\n",
      "357 oliver.txt\n",
      "358 legal.hum\n",
      "359 resolutn.txt\n",
      "360 namaste.txt\n",
      "361 insanity.hum\n",
      "362 bredcake.des\n",
      "363 mutate.hum\n",
      "364 recipe.003\n",
      "365 bad\n",
      "366 mrsfield\n",
      "368 hotel.txt\n",
      "369 poopie.txt\n",
      "370 recipe.004\n",
      "372 blackhol.hum\n",
      "373 blake7.lis\n",
      "374 buzzword.hum\n",
      "375 catranch.hum\n",
      "376 wimptest.txt\n",
      "377 court.quips\n",
      "378 cast.lis\n",
      "379 supermar.rul\n",
      "380 insuranc.sty\n",
      "381 truths.hum\n",
      "384 jc-elvis.inf\n",
      "385 terbear.txt\n",
      "386 woodbugs.txt\n",
      "388 jac&tuu.hum\n",
      "389 beer-gui\n",
      "390 hbo_spec.rev\n",
      "391 rns_ency.txt\n",
      "392 beapimp.hum\n",
      "393 beerwarn.txt\n",
      "394 gd_guide.txt\n",
      "395 adameve.hum\n",
      "397 llong.hum\n",
      "398 quick.jok\n",
      "399 zucantom.sal\n",
      "400 libraway.txt\n",
      "401 curry.hrb\n",
      "402 bigpic1.hum\n",
      "403 worldend.hum\n",
      "405 engineer.hum\n",
      "406 interv.hum\n",
      "407 letgosh.txt\n",
      "408 pickup.txt\n",
      "409 eggroll1.mea\n",
      "410 termpoem.txt\n",
      "411 footfun.hum\n",
      "412 butcher.txt\n",
      "413 coladrik.txt\n",
      "414 sanshop.txt\n",
      "416 red-neck.jks\n",
      "417 sysadmin.txt\n",
      "418 beergame.hum\n",
      "420 lawskool.txt\n",
      "421 dromes.txt\n",
      "422 hitler.txt\n",
      "423 cops.txt\n",
      "424 oatbran.rec\n",
      "425 gohome.hum\n",
      "426 taping.hum\n",
      "427 three.txt\n",
      "428 paddingurpapers.txt\n",
      "430 spacever.hum\n",
      "432 trukdeth.txt\n",
      "433 history2.oop\n",
      "435 richbred.txt\n",
      "436 chickens.txt\n",
      "437 pasta001.sal\n",
      "438 teevee.hum\n",
      "439 smiley.txt\n",
      "440 gd_drwho.txt\n",
      "441 hecomes.jok\n",
      "442 thecube.hum\n",
      "443 beershrp.fis\n",
      "444 billcat.hum\n",
      "445 reconcil.hum\n",
      "446 fudge.txt\n",
      "447 unochili.txt\n",
      "448 feggmagi.txt\n",
      "449 modstup\n",
      "450 cockney.alp\n",
      "451 languag.jok\n",
      "452 deep.txt\n",
      "453 humatran.jok\n",
      "454 bread.txt\n",
      "455 miami.hum\n",
      "458 enlightenment.txt\n",
      "459 hack\n",
      "460 jambalay.pol\n",
      "461 brush1.txt\n",
      "462 awespinh.sal\n",
      "463 jon.txt\n",
      "464 goldwatr.txt\n",
      "465 word.hum\n",
      "466 polly.txt\n",
      "467 kid2\n",
      "468 shooters.txt\n",
      "469 antimead.bev\n",
      "470 beer-g\n",
      "471 how2dotv.txt\n",
      "472 desk.txt\n",
      "473 cowexplo.hum\n",
      "474 report.hum\n",
      "475 boston.geog\n",
      "476 crazy.txt\n",
      "477 bugs.txt\n",
      "478 rockmus.hum\n",
      "479 smurfs.cc\n",
      "480 smartass.txt\n",
      "481 epi_merm.txt\n",
      "482 hangover.txt\n",
      "483 lll.hum\n",
      "484 beesherb.txt\n",
      "485 psych_pr.quo\n",
      "486 deathhem.txt\n",
      "489 barney.cn1\n",
      "490 lion.txt\n",
      "491 prooftec.txt\n",
      "492 msorrow\n",
      "493 teens.txt\n",
      "494 wrdnws8.txt\n",
      "495 burrito.mea\n",
      "496 onan.txt\n",
      "497 topten.hum\n",
      "498 cmu.share\n",
      "499 ratspit.hum\n",
      "500 abbott.txt\n",
      "501 nysucks.hum\n",
      "503 twilight.txt\n",
      "504 marriage.hum\n",
      "505 stagline.txt\n",
      "506 iced.tea\n",
      "507 psilaine.hum\n",
      "509 princess.brd\n",
      "510 applepie.des\n",
      "511 wagon.hum\n",
      "512 seeds42.txt\n",
      "513 apsaucke.des\n",
      "514 terrnieg.hum\n",
      "515 bond-2.txt\n",
      "516 pipespec.txt\n",
      "517 ayurved.txt\n",
      "518 strattma.txt\n",
      "519 gd_sgrnd.txt\n",
      "520 leech.txt\n",
      "521 exylic.txt\n",
      "522 insect1.txt\n",
      "523 suicide2.txt\n",
      "524 cartoon.laws\n",
      "525 roach.asc\n",
      "526 bagelope.txt\n",
      "527 missheav.hum\n",
      "528 cooplaws\n",
      "529 sorority.gir\n",
      "530 figure_1.txt\n",
      "531 radiolaf.hum\n",
      "532 lawhunt.txt\n",
      "533 wetdream.hum\n",
      "534 hackingcracking.txt\n",
      "535 snapple.rum\n",
      "536 critic.txt\n",
      "537 murphy.txt\n",
      "538 oam-001.txt\n",
      "540 myheart.hum\n",
      "541 skippy.hum\n",
      "542 makebeer.hum\n",
      "543 netmask.txt\n",
      "544 dingding.hum\n",
      "545 y.txt\n",
      "546 prawblim.hum\n",
      "547 lipkovits.txt\n",
      "548 johann\n",
      "549 dalive\n",
      "550 coke_fan.naz\n",
      "551 lansing.txt\n",
      "552 packard.txt\n",
      "553 jrrt.riddle\n",
      "554 herb!.hum\n",
      "555 basehead.txt\n",
      "556 foodtips\n",
      "557 luzerzo2.hum\n",
      "559 wrdnws9.txt\n",
      "561 social.hum\n",
      "562 badday.hum\n",
      "563 idaho.txt\n",
      "564 ppbeer.txt\n",
      "565 luvstory.txt\n",
      "566 appetiz.rcp\n",
      "567 mash.hum\n",
      "569 dthought.txt\n",
      "570 repair.hum\n",
      "571 idr2.txt\n",
      "572 kashrut.txt\n",
      "573 record_.gap\n",
      "574 cucumber.txt\n",
      "575 elevator.fun\n",
      "576 charity.hum\n",
      "577 woodsmok.txt\n",
      "578 quantum.phy\n",
      "579 wrdnws4.txt\n",
      "580 python_s.ong\n",
      "581 variety2.asc\n",
      "582 booze2.fun\n",
      "583 all_grai\n",
      "584 failure.txt\n",
      "585 cform2.txt\n",
      "586 orgfrost.bev\n",
      "587 problem.txt\n",
      "588 arnold.txt\n",
      "589 golnar.txt\n",
      "590 coldfake.hum\n",
      "591 advrtize.txt\n",
      "593 number\n",
      "594 fartinfo.txt\n",
      "595 econridl.fun\n",
      "596 coollngo2.txt\n",
      "597 nigel10.txt\n",
      "598 booknuti.txt\n",
      "599 mog-history\n",
      "600 inlaws1.txt\n",
      "601 labels.txt\n",
      "602 news.hum\n",
      "603 ripoffpc.hum\n",
      "605 moonshin\n",
      "607 brainect.hum\n",
      "609 just2\n",
      "610 rentals.hum\n",
      "612 rapmastr.hum\n",
      "613 bless.bc\n",
      "614 weights.hum\n",
      "616 yuban.txt\n",
      "617 christop.int\n",
      "618 cancer.rat\n",
      "619 berryeto.bev\n",
      "620 quotes.bug\n",
      "621 computer.txt\n",
      "623 thesis.beh\n",
      "624 top10.elf\n",
      "626 polemom.txt\n",
      "628 final-ex.txt\n",
      "629 variety3.asc\n",
      "630 fegg!int.txt\n",
      "631 clancy.txt\n",
      "632 wrdnws5.txt\n",
      "633 parabl.hum\n",
      "634 hell.jok\n",
      "635 parades.hum\n",
      "636 fbipizza.txt\n",
      "637 quest.hum\n",
      "638 nukewar.txt\n",
      "639 farsi.phrase\n",
      "640 byfb.txt\n",
      "641 swearfrn.hum\n",
      "642 cartoon.law\n",
      "643 motrbike.jok\n",
      "644 solders.hum\n",
      "645 spider.hum\n",
      "647 calamus.hrb\n",
      "648 english\n",
      "649 ohandre.hum\n",
      "651 odd_to.obs\n",
      "652 slogans.txt\n",
      "653 univ.odd\n",
      "656 inquirer.txt\n",
      "657 nintendo.jok\n",
      "658 booze1.fun\n",
      "659 renored.txt\n",
      "660 fireplacein.txt\n",
      "661 skincat\n",
      "662 d-ned.hum\n",
      "663 women.jok\n",
      "664 admin.txt\n",
      "665 rocking.hum\n",
      "666 steroid.txt\n",
      "667 bugbreak.hum\n",
      "668 whoon1st.hum\n",
      "669 nigel.10\n",
      "670 indgrdn.txt\n",
      "671 wisdom\n",
      "672 bible.txt\n",
      "673 texbeef.txt\n",
      "674 gd_liqtv.txt\n",
      "675 primes.jok\n",
      "676 banana04.brd\n",
      "677 woodbine.txt\n",
      "678 lawyers.txt\n",
      "679 banana05.brd\n",
      "681 allusion\n",
      "682 nasaglenn.txt\n",
      "683 trekwes.hum\n",
      "684 adt_miam.txt\n",
      "685 oranchic.pol\n",
      "686 butstcod.fis\n",
      "688 epitaph\n",
      "689 boatmemo.jok\n",
      "691 how_to_i.pro\n",
      "692 gameshow.txt\n",
      "693 mitch.txt\n",
      "695 bw.txt\n",
      "696 cold.fus\n",
      "697 alcatax.txt\n",
      "699 cooking.jok\n",
      "700 whatbbs\n",
      "701 firstaid.txt\n",
      "702 godmonth.txt\n",
      "703 coyote.txt\n",
      "704 cabbage.txt\n",
      "706 beer.gam\n",
      "707 resrch_p.hra\n",
      "708 washroom.txt\n",
      "710 devils.jok\n",
      "711 a_fish_c.apo\n",
      "713 gingbeer.txt\n",
      "714 squids.gph\n",
      "715 engmuffn.txt\n",
      "716 rns_bwl.txt\n",
      "717 talkbizr.txt\n",
      "718 kid_diet.txt\n",
      "719 widows\n",
      "720 grammar.jok\n",
      "721 boarchil.txt\n",
      "722 dieter.txt\n",
      "723 sysman.txt\n",
      "724 bitchcar.hum\n",
      "725 bunacald.fis\n",
      "726 wrdnws2.txt\n",
      "727 donut.txt\n",
      "729 textgrap.hum\n",
      "730 vaguemag.90s\n",
      "731 pracjoke.txt\n",
      "732 oldeng.hum\n",
      "733 nigel.7\n",
      "734 men&wome.txt\n",
      "735 diet.txt\n",
      "736 religion.txt\n",
      "737 diesmurf.txt\n",
      "738 caesardr.sal\n",
      "739 freudonseuss.txt\n",
      "740 banana01.brd\n",
      "742 zen.txt\n",
      "743 catballs.hum\n",
      "744 hum2\n",
      "745 initials.rid\n",
      "746 poli.tics\n",
      "747 realest.txt\n",
      "748 oldtime.txt\n",
      "750 buffwing.pol\n",
      "752 oldtime.sng\n",
      "753 snipe.txt\n",
      "754 gd_frasr.txt\n",
      "755 confucius_say.txt\n",
      "756 aphrodis.txt\n",
      "757 o-ttalk.hum\n",
      "758 x-drinks.txt\n",
      "759 racist.net\n",
      "760 let.go\n",
      "761 puzzles.jok\n",
      "762 rns_bcl.txt\n",
      "763 venison.txt\n",
      "764 heroic.txt\n",
      "765 arcadian.txt\n",
      "767 ivan.hum\n",
      "768 drinks.txt\n",
      "769 temphell.jok\n",
      "770 outlimit.txt\n",
      "771 fearcola.hum\n",
      "772 food\n",
      "773 co-car.jok\n",
      "774 mydaywss.hum\n",
      "775 moore.txt\n",
      "776 beergame.txt\n",
      "778 shuttleb.hum\n",
      "779 art-fart.hum\n",
      "780 nzdrinks.txt\n",
      "781 capital.txt\n",
      "783 nameisreo.txt\n",
      "784 english.txt\n",
      "785 puzzle.spo\n",
      "786 timetr.hum\n",
      "787 watchlip.hum\n",
      "788 epi_tton.txt\n",
      "789 normalboy.txt\n",
      "790 terms.hum\n",
      "791 mothers.txt\n",
      "792 math.1\n",
      "793 recipe.012\n",
      "794 wrdnws1.txt\n",
      "796 gown.txt\n",
      "797 simp.txt\n",
      "798 age.txt\n",
      "799 jerky.rcp\n",
      "800 test2.jok\n",
      "801 beershrm.fis\n",
      "803 reddye.hum\n",
      "804 skippy.txt\n",
      "805 psalm23.txt\n",
      "806 drugshum.hum\n",
      "807 hedgehog.txt\n",
      "808 standard.hum\n",
      "809 oxymoron.jok\n",
      "810 odearakk.hum\n",
      "811 impurmat.hum\n",
      "812 bnb_quot.txt\n",
      "813 gumbo.txt\n",
      "814 madscrib.hum\n",
      "815 banana02.brd\n",
      "816 hack7.txt\n",
      "817 pukeprom.jok\n",
      "818 banana03.brd\n",
      "819 a_tv_t-p.com\n",
      "820 rinaldos.law\n",
      "821 anime.lif\n",
      "822 livnware.hum\n",
      "823 thievco.txt\n",
      "824 damiana.hrb\n",
      "825 drinking.tro\n",
      "826 relative.ada\n",
      "827 cheapfar.hum\n",
      "828 staff.txt\n",
      "829 childhoo.jok\n",
      "830 japrap.hum\n",
      "831 rent-a_cat\n",
      "832 acetab1.txt\n",
      "833 nuke.hum\n",
      "834 curiousgeorgie.txt\n",
      "835 chinesec.hum\n",
      "836 engrhyme.txt\n",
      "837 candybar.fun\n",
      "838 coke.txt\n",
      "839 jawsalad.fis\n",
      "840 deterior.hum\n",
      "841 voltron.hum\n",
      "842 pol-corr.txt\n",
      "843 imprrisk.hum\n",
      "844 proof.met\n",
      "845 japice.bev\n",
      "846 revolt.dj\n",
      "847 boe.hum\n",
      "849 policpig.hum\n",
      "850 shameonu.hum\n",
      "851 recip1.txt\n",
      "852 netnews.10\n",
      "853 lucky.cha\n",
      "854 disaster.hum\n",
      "855 tuna.lab\n",
      "856 vonthomp\n",
      "857 renorthr.txt\n",
      "858 oam.nfo\n",
      "859 manspace.hum\n",
      "860 dark.suc\n",
      "862 kaboom.hum\n",
      "863 horoscop.txt\n",
      "864 anthropo.stu\n",
      "865 pepsideg.txt\n",
      "866 jokes1.txt\n",
      "867 amchap2.txt\n",
      "869 druggame.hum\n",
      "870 tshirts.jok\n",
      "871 waitress.txt\n",
      "873 epiquest.txt\n",
      "874 units.mea\n",
      "875 pot.txt\n",
      "876 fuckyou2.txt\n",
      "878 marines.hum\n",
      "879 greenchi.txt\n",
      "880 jokes.txt\n",
      "882 trekfume.txt\n",
      "883 popconc.hum\n",
      "884 f_tang.txt\n",
      "885 socecon.hum\n",
      "886 letterbx.txt\n",
      "888 t_zone.jok\n",
      "889 calvin.txt\n",
      "890 hop.faq\n",
      "891 namm\n",
      "892 poets.hum\n",
      "893 cookberk\n",
      "894 booze.fun\n",
      "895 eskimo.nel\n",
      "896 epi_rns.txt\n",
      "897 att.txt\n",
      "898 browneco.hum\n",
      "899 breadpud.des\n",
      "900 flattax.hum\n",
      "901 change.hum\n",
      "903 pepper.txt\n",
      "904 recipe.001\n",
      "905 montoys.txt\n",
      "908 humpty.dumpty\n",
      "909 parsnip.txt\n",
      "910 cogdis.txt\n",
      "911 venganza.txt\n",
      "913 outawork.erl\n",
      "914 beerdiag.txt\n",
      "915 climbing.let\n",
      "916 madhattr.jok\n",
      "917 bhang.fun\n",
      "918 petshop\n",
      "919 recipe.006\n",
      "920 childrenbooks.txt\n",
      "921 weight.txt\n",
      "923 maecenas.hum\n",
      "924 mailfrag.hum\n",
      "925 hermsys.txt\n",
      "926 gas.txt\n",
      "927 acronym.txt\n",
      "928 acne1.txt\n",
      "929 recipe.008\n",
      "931 bozo_tv.leg\n",
      "934 jalapast.dip\n",
      "935 spelin_r.ifo\n",
      "936 pun.txt\n",
      "937 sf-zine.pub\n",
      "938 apsnet.txt\n",
      "939 residncy.jok\n",
      "940 kloo.txt\n",
      "941 whatthe.hum\n",
      "942 gd_maxhd.txt\n",
      "943 office.txt\n",
      "944 dead4.txt\n",
      "946 minn.txt\n",
      "948 antibiot.txt\n",
      "949 talebeat.hum\n",
      "950 howlong.hum\n",
      "951 ratings.hum\n",
      "952 bread.rec\n",
      "953 proudlyserve.txt\n",
      "955 research.hum\n",
      "956 peanuts.txt\n",
      "957 penndtch\n",
      "958 mcd.txt\n",
      "959 t-10.hum\n",
      "960 recipe.009\n",
      "961 psalm.reagan\n",
      "962 tribble.hum\n",
      "963 church.sto\n",
      "964 freshman.hum\n",
      "965 ins1\n",
      "966 college.txt\n",
      "967 lazarus.txt\n",
      "968 psalm_nixon\n",
      "969 dandwine.bev\n",
      "970 rinaldo.jok\n",
      "971 valujet.txt\n",
      "972 nurds.hum\n",
      "973 grommet.hum\n",
      "974 telecom.q\n",
      "975 rinaldos.txt\n",
      "977 gotukola.hrb\n",
      "978 back1.txt\n",
      "979 lifeimag.hum\n",
      "980 atherosc.txt\n",
      "981 addrmeri.txt\n",
      "982 speling.msk\n",
      "983 caramels.des\n",
      "984 appbred.brd\n",
      "985 normquot.txt\n",
      "986 lampoon.jok\n",
      "987 recipe.007\n",
      "988 element.jok\n",
      "989 q.pun\n",
      "990 drinker.txt\n",
      "991 italoink.txt\n",
      "992 episimp2.txt\n",
      "993 alcohol.hum\n",
      "994 why-me.hum\n",
      "995 newconst.hum\n",
      "996 happyhack.txt\n",
      "997 homermmm.txt\n",
      "998 lines.jok\n",
      "999 catstory.txt\n",
      "1001 arab.dic\n",
      "1002 bingbong.hum\n",
      "1003 b12.txt\n",
      "1004 stressman.txt\n",
      "1008 hotnnot.hum\n",
      "1009 dover.poem\n",
      "1010 b-2.jok\n",
      "1012 making_y.wel\n",
      "1013 chickens.jok\n",
      "1014 phony.hum\n",
      "1015 xtermin8.hum\n",
      "1016 coke.fun\n",
      "1017 yogisays.txt\n",
      "1018 sawyer.txt\n",
      "1020 st_silic.txt\n",
      "1021 hitchcoc.app\n",
      "1022 woods.txt\n",
      "1023 truthlsd.hum\n",
      "1024 twinpeak.txt\n",
      "1026 iqtest\n",
      "1027 hitlerap.txt\n",
      "1029 aniherb.txt\n",
      "1030 blaster.hum\n",
      "1031 grail.txt\n",
      "1032 ourfathr.txt\n",
      "1033 chunnel.txt\n",
      "1034 recepies.fun\n",
      "1035 firecamp.txt\n",
      "1036 curry.txt\n",
      "1037 number_k.ill\n",
      "1038 anorexia.txt\n",
      "1039 bad-d\n",
      "1040 garlpast.vgn\n",
      "1041 growth.txt\n",
      "1042 kilsmur.hum\n",
      "1043 doc-says.txt\n",
      "1044 losers84.hum\n",
      "1045 jargon.phd\n",
      "1046 cbmatic.hum\n",
      "1047 manager.txt\n",
      "1048 adcopy.hum\n",
      "1050 welfare.txt\n",
      "1051 startrek.txt\n",
      "1052 hilbilly.wri\n",
      "1053 bhb.ill\n",
      "1054 miamadvi.hum\n",
      "1055 turbo.hum\n",
      "1056 luggage.hum\n",
      "1057 facedeth.txt\n",
      "1058 epikarat.txt\n",
      "1060 blkbnsrc.vgn\n",
      "1061 stereo.txt\n",
      "1062 icm.hum\n",
      "1063 beer.hum\n",
      "1064 how.bugs.breakd\n",
      "1065 mowers.txt\n",
      "1066 fiber.txt\n",
      "1067 cake.rec\n",
      "1068 beerjesus.hum\n",
      "1069 egg-bred.txt\n",
      "1070 catin.hat\n",
      "1071 harmful.hum\n",
      "1072 poll2res.hum\n",
      "1073 draxamus.txt\n",
      "1074 lozeuser.hum\n",
      "1076 testchri.txt\n",
      "1078 ludeinfo.txt\n",
      "1079 la_times.hun\n",
      "1080 beginn.ers\n",
      "1082 focaccia.brd\n",
      "1083 headlnrs\n",
      "1085 twinkie.txt\n",
      "1086 maxheadr\n",
      "1087 conan.txt\n",
      "1088 flowchrt.txt\n",
      "1089 lion.jok\n",
      "1090 airlines\n",
      "1091 popmusi.hum\n",
      "1093 empeval.txt\n",
      "1094 resrch_phrase\n",
      "1095 shorties.jok\n",
      "1097 height.txt\n",
      "1098 gaiahuma\n",
      "1099 reagan.hum\n",
      "1100 phorse.hum\n",
      "1101 yjohncse.hum\n",
      "1102 dym\n",
      "1103 eatme.txt\n",
      "1104 losers86.hum\n",
      "1105 cartwb.son\n",
      "1106 saveface.hum\n",
      "1107 normal.boy\n",
      "1108 modest.hum\n",
      "1109 oculis.rcp\n",
      "1110 soleleer.hum\n",
      "1111 gd_ol.txt\n",
      "1112 zodiac.hum\n",
      "1113 beave.hum\n",
      "1114 seafood.txt\n",
      "1116 kilroy\n",
      "1117 jawgumbo.fis\n",
      "1118 t-shirt.hum\n",
      "1119 feista01.dip\n",
      "1121 fascist.txt\n",
      "1122 bored.txt\n",
      "1123 contract.moo\n",
      "1124 necropls.txt\n",
      "1125 liceprof.sty\n",
      "1126 soporifi.abs\n",
      "1127 earp\n",
      "1128 televisi.txt\n",
      "1129 jungjuic.bev\n",
      "1130 smurf-03.txt\n"
     ]
    }
   ],
   "source": [
    "for f in x:\n",
    "    print(f, fileMap[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe57541",
   "metadata": {},
   "source": [
    "# Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37273c",
   "metadata": {},
   "source": [
    "## a) Preprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess2(file):\n",
    "    # Convert file to lowercase.\n",
    "    file = file.lower()\n",
    "    \n",
    "    # Tokenize and remove stopwords.\n",
    "    file = \" \".join([word for word in file.split() if word not in stopWords])\n",
    "    \n",
    "    # word tokenize.\n",
    "    file = word_tokenize(file)\n",
    "    \n",
    "    # Remove stopwords remaining\n",
    "    file = [word for word in file if word not in stopWords]\n",
    "    \n",
    "    # Remove punctuations\n",
    "    file = [x.translate(str.maketrans('', '', string.punctuation)) for x in file]\n",
    "    \n",
    "    # Remove len <= 2 tokens and lemmatize the words.\n",
    "    newFile = []\n",
    "    for word in file:\n",
    "        word = word.strip()\n",
    "        if len(word) > 2:\n",
    "            newFile.append(lemmatizer.lemmatize(word))\n",
    "    return newFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5065f22",
   "metadata": {},
   "source": [
    "## b) Positional Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "positionalIndex = {}\n",
    "for idx, file in enumerate(files):\n",
    "    file = preProcess2(file)\n",
    "    for wordIdx, word in enumerate(file):\n",
    "        # Word not existed before.\n",
    "        if word not in positionalIndex:\n",
    "            positionalIndex[word] = {}\n",
    "            positionalIndex[word][\"docList\"] = [idx]\n",
    "            \n",
    "        # Word existed, but occuring first time in this document.\n",
    "        if idx not in positionalIndex[word]:\n",
    "            positionalIndex[word][idx] = []\n",
    "            \n",
    "        # Insert the word location for this document.\n",
    "        positionalIndex[word][idx].append(wordIdx)\n",
    "        if positionalIndex[word][\"docList\"][-1] != idx:\n",
    "            positionalIndex[word][\"docList\"].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162308b9",
   "metadata": {},
   "source": [
    "## c) Phrase Query Function:\n",
    "\n",
    "##### Phrase query support for <= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84eaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPositionalList(word):\n",
    "    if word not in positionalIndex:\n",
    "        return {\n",
    "            \"docList\": []\n",
    "        }\n",
    "    return positionalIndex[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30751c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takePhraseInput():\n",
    "    q = input(\"Enter query: \")\n",
    "    q = preProcess2(q)\n",
    "    print(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPhraseQueryResult(query):\n",
    "    if len(query) == 0:\n",
    "        return (0, [])\n",
    "    \n",
    "    # get query for a single word\n",
    "    xList = getPositionalList(query[0])\n",
    "    \n",
    "    # initialize answer.\n",
    "    ansList = xList[\"docList\"]\n",
    "    \n",
    "    # for next words.\n",
    "    for word in query[1:]:\n",
    "        yList = getPositionalList(word)\n",
    "        # get common documents.\n",
    "        commonDocs = xANDy(xList[\"docList\"], yList[\"docList\"])[0]\n",
    "        newList = []\n",
    "        \n",
    "        # in common documents check if words are adjacent.\n",
    "        for docId in commonDocs:\n",
    "            # get positions of the words in this docId\n",
    "            xPos = xList[docId]\n",
    "            yPos = yList[docId]\n",
    "            \n",
    "            a = 0\n",
    "            b = 0\n",
    "            \n",
    "            while a < len(xPos) and b < len(yPos):\n",
    "                if xPos[a] == yPos[b]-1:\n",
    "                    newList.append(docId)\n",
    "                    a += 1\n",
    "                    b += 1\n",
    "                    break\n",
    "                elif xPos[a] < yPos[b]:\n",
    "                    a += 1\n",
    "                else:\n",
    "                    b += 1\n",
    "            \n",
    "        xList = yList\n",
    "        \n",
    "        ansList = xANDy(ansList, newList)[0]\n",
    "    \n",
    "    return (len(ansList), ansList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = takePhraseInput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = getPhraseQueryResult(q)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in res[1]:\n",
    "    print(fileMap[filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = takePhraseInput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = getPhraseQueryResult(q)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in res[1]:\n",
    "    print(fileMap[filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = takePhraseInput()\n",
    "res = getPhraseQueryResult(q)\n",
    "print(res)\n",
    "for filename in res[1]:\n",
    "    print(fileMap[filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96eeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1d226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d54909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
